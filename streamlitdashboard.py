# -*- coding: utf-8 -*-
# Author : Dr. Songhee Kang
# Description : KOSTEC stat visualizer using Excel-based trend summary

import streamlit as st
import pandas as pd
import altair as alt
from streamlit_agraph import agraph, Node, Edge, Config
from collections import defaultdict
import os, io, zipfile
from datetime import date, datetime
import glob, time

# --- 1. ì„¤ì •
st.set_page_config(page_title="í•œì¤‘ê³¼ê¸°í˜‘ë ¥ì„¼í„° í‚¤ì›Œë“œ ëŒ€ì‹œë³´ë“œ", layout="wide")

# --- 2. CSS ì ìš©
def local_css(file_name):
    with open(file_name, "r", encoding="utf-8") as f:
        css_content = f.read()
    st.markdown(f"<style>{css_content}</style>", unsafe_allow_html=True)

local_css("assets/css/main.css")

# --- 3. ì‚¬ì´ë“œë°” 
input_date = st.sidebar.date_input("ğŸ“† ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ", value=date.today())
api_token = st.sidebar.text_input("ğŸ” ìˆ˜ì§‘ ì•”í˜¸ ì…ë ¥", type="password")
github_token = st.sidebar.text_input("ğŸªª ì—…ë¡œë“œ ì•”í˜¸ ì…ë ¥", type="password")

current_date = input_date.strftime("%Y%m%d")

if st.sidebar.button("ğŸ›° ì£¼ê°„ ë™í–¥ ìˆ˜ì§‘ ì‹œì‘"):
    with st.spinner("â³ ì£¼ê°„ ë™í–¥ì„ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤. ìµœëŒ€ 3~5ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤..."):
        try:
            import os
            import anthropic
            import re
            from io import StringIO
            from itertools import combinations
            from openpyxl import load_workbook
       
                # API ì—°ê²°
            client = anthropic.Anthropic(api_key=api_token)
        
            with open("assets/input/keywords.txt", "r", encoding="utf-8") as f:
                keywords = f.read().strip()
            with open("assets/input/en_keywords.txt", "r", encoding="utf-8") as f:
                en_keywords = f.read().strip()
            with open("assets/input/sites.txt", "r", encoding="utf-8") as f:
                source_sites = f.read().strip()
            with open("assets/input/prompt.txt", "r", encoding="utf-8") as f:
                prompt_template = f.read()
               
                # ë³€ìˆ˜ ì •ì˜
            prompt1 = prompt_template.format(
                keywords=keywords,        # ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ joiní•œ ê°’
                current_date=current_date,        # '20250518' ê°™ì€ ë¬¸ìì—´
                source_sites=source_sites        # ë¬¸ìì—´ ë˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡
            )
            #st.write(prompt1)
            prompt2 = prompt_template.format(
                keywords=en_keywords,        # ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ joiní•œ ê°’
                current_date=current_date,        # '20250518' ê°™ì€ ë¬¸ìì—´
                source_sites="*"        # ë¬¸ìì—´ ë˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡
            )
            #st.write(prompt2)
                # Claude API í˜¸ì¶œ
            message = client.messages.create(
                model="claude-3-7-sonnet-20250219",
                max_tokens=20000,
                temperature=1,
                messages=[{"role": "user", "content": [{"type": "text", "text": prompt1}]}]
            )
    
            #st.write(prompt1)
            
                # ê²°ê³¼ íŒŒì‹±
            text_data = message.content[0].text if isinstance(message.content, list) else message.content.text
            match = re.search(r"<excel_report>(.*?)</excel_report>", text_data, re.DOTALL)
            text_block = match.group(0) if match else None
        
            sheet1_start = text_block.find("<sheet1>")
            sheet1_end = text_block.find("</sheet1>")
            sheet2_start = text_block.find("<sheet2>")
            sheet2_end = text_block.find("</sheet2>")
            summary_start = text_block.find("<executive_summary>")
            summary_end = text_block.find("</executive_summary>")
        
            sheet1_text = text_block[sheet1_start:sheet1_end]
            sheet2_text = text_block[sheet2_start:sheet2_end]
            executive_summary_text = text_block[summary_start + len("<executive_summary>"):summary_end].strip()
        
            sheet1_table_match = re.search(r"(\|.+?\|\n\|[-|]+\|(?:\n\|.*?\|)+)", sheet1_text)
            sheet2_table_match = re.search(r"(\|.+?\|\n\|[-|]+\|(?:\n\|.*?\|)+)", sheet2_text)
        
            sheet1_table_md = sheet1_table_match.group(1).strip() if sheet1_table_match else ""
            sheet2_table_md = sheet2_table_match.group(1).strip() if sheet2_table_match else ""
        
            df_sheet1 = pd.read_csv(StringIO(sheet1_table_md), sep="|", engine="python").dropna(axis=1, how="all")
            df_sheet2 = pd.read_csv(StringIO(sheet2_table_md), sep="|", engine="python").dropna(axis=1, how="all")
        
                # ì €ì¥
            excel_path = f"assets/data/{current_date}_trend_summary.xlsx"
                # ë™ì‹œì¶œí˜„ ë° ì—°ê´€ì–´ ë¶„ì„
            df_summary = df_sheet1.iloc[1:].reset_index(drop=True)
            df_summary.columns = [col.strip() for col in df_summary.columns]
            keywords_list = [kw.strip() for kw in df_summary["Keyword"].dropna().unique().tolist()]
        
            cooccur_counter = defaultdict(int)
            association_counter = defaultdict(int)
            for _, row in df_summary.iterrows():
                text = (str(row.get("Detailed Summary", "")) + " " + str(row.get("Short Summary", ""))).lower()
                present_keywords = [kw for kw in keywords_list if kw.lower() in text]
                for kw1, kw2 in combinations(sorted(set(present_keywords)), 2):
                    cooccur_counter[(kw1, kw2)] += 1
                for kw in present_keywords:
                    association_counter[kw] += 1
        
            df_cooccur = pd.DataFrame([{"source": k1, "target": k2, "count": v} for (k1, k2), v in cooccur_counter.items()])
            df_association = pd.DataFrame([{"term": k, "count": v} for k, v in association_counter.items()])
        
            with pd.ExcelWriter(excel_path, engine="openpyxl", mode="w") as writer:
                df_summary.to_excel(writer, index=False, sheet_name="Summary Table")
                df_sheeet2.to_excel(writer, index=False, sheet_name="Sources")
                pd.DataFrame({"Executive Summary": [executive_summary_text]}).to_excel(writer, index=False, sheet_name="Executive Summary")
                df_cooccur.to_excel(writer, index=False, sheet_name="Cooccurrence")
                df_association.to_excel(writer, index=False, sheet_name="Associations")
        
            st.sidebar.success(f"{current_date} ê¸°ì¤€ ì£¼ê°„ ë™í–¥ ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ!")
        
        except Exception as e:
        	st.sidebar.error(f"âŒ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
        from github import Github
        repo_name = "kostec-stat/streamlit"
        file_path = f"assets/data/{current_date}_trend_summary.xlsx"
            
        try:
            g = Github(github_token)
            repo = g.get_repo(repo_name)
            
            with open(file_path, "rb") as f:
                content = f.read()
            path_in_repo = f"assets/data/{current_date}_trend_summary.xlsx"
            
            try:
                existing_file = repo.get_contents(path_in_repo)
                repo.update_file(existing_file.path, f"update {path_in_repo}", content, existing_file.sha)
            except Exception:
                repo.create_file(path_in_repo, f"add {path_in_repo}", content)
            
            st.success(f" {current_date} ê¸°ì¤€ ì£¼ê°„ ë™í–¥ ìˆ˜ì§‘, ì €ì¥ ë° GitHub ì—…ë¡œë“œ ì™„ë£Œ!")
        except Exception as upload_err:
            st.warning(f"âš ï¸ ìˆ˜ì§‘ì€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ GitHub ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_err}")
                
st.sidebar.markdown("---")
snapshot_files = glob.glob("assets/data/*_trend_summary.xlsx")
snapshot_dates = sorted({os.path.basename(f).split("_")[0] for f in snapshot_files})
selected_snapshot = st.sidebar.selectbox("ğŸ“… ìŠ¤ëƒ…ìƒ· ë‚ ì§œ ì„ íƒ", snapshot_dates)
excel_path = f"assets/data/{selected_snapshot}_trend_summary.xlsx"

@st.cache_data
def load_excel_data(path):
    xls = pd.ExcelFile(path)
    df_summary = pd.read_excel(xls, sheet_name="Summary Table")
    df_sources = pd.read_excel(xls, sheet_name="Sources")
    df_exec = pd.read_excel(xls, sheet_name="Executive Summary")
    df_cooccur = pd.read_excel(xls, sheet_name="Cooccurrence")
    df_assoc = pd.read_excel(xls, sheet_name="Associations")
    return df_summary, df_sources, df_exec, df_cooccur, df_assoc

try:
    df_summary, df_sources, df_exec, df_cooccur, df_assoc = load_excel_data(excel_path)
except Exception as e:
    st.error(f"ë¶„ì„ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    st.stop()

df_summary.columns = [col.strip() for col in df_summary.columns]
df_cooccur.columns = [col.strip() for col in df_cooccur.columns]

if "count" not in df_cooccur.columns:
    st.error("âŒ 'count' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.write("ğŸ“Œ í˜„ì¬ ì»¬ëŸ¼:", df_cooccur.columns.tolist())
    st.stop()

# ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ì¸ì§€ í™•ì¸
if "Keyword Count" not in df_summary.columns:
    st.error("âŒ 'Keyword Count' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.write("ğŸ” í˜„ì¬ ì»¬ëŸ¼ ëª©ë¡:", df_summary.columns.tolist())
    st.stop()
# 1. ì—‘ì…€ì—ì„œ ì‹œíŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
xls = pd.ExcelFile(excel_path)
df_summary = pd.read_excel(xls, sheet_name="Summary Table")
df_sources = pd.read_excel(xls, sheet_name="Sources")

# 2. ì»¬ëŸ¼ëª… ì •ë¦¬
df_summary.columns = [c.strip() for c in df_summary.columns]
df_sources.columns = [c.strip() for c in df_sources.columns]

# 3. URL ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ë§¤í•‘
df_merged = df_summary.merge(
    df_sources[["URL", "Publication Date"]],
    how="left",
    left_on="Source URL",
    right_on="URL"
)
# 4. ë‚ ì§œ ì •ë¦¬
df_merged["Publication Date"] = pd.to_datetime(df_merged["Publication Date"])
df_merged["Keyword"] = df_merged["Keyword"].astype(str)

# 5. ì¼ìë³„ í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ ì§‘ê³„
df_daily = df_merged.groupby(["Publication Date", "Keyword"]).size().reset_index(name="count")

# 6. í”¼ë²— í…Œì´ë¸”ë¡œ ì¼ì x í‚¤ì›Œë“œ í˜•íƒœ
df_pivot = df_daily.pivot_table(index="Publication Date", columns="Keyword", values="count", fill_value=0).sort_index()

# 7. 7ì¼ ì´ë™ í‰ê· 
df_rolling = df_pivot.rolling(window=7, min_periods=1).mean()

# --- 4. íƒ­ êµ¬ì„±
tab1, tab2, tab3, tab4 = st.tabs([
    "ğŸ“Š ì£¼ê°„ìš”ì•½ê³¼ ë‹¤ìš´ë¡œë“œ", 
    "ğŸ•¸ ë™ì‹œì¶œí˜„ê³¼ ì—°ê´€ì–´", 
    "ğŸ” ë¹ˆë„ìˆ˜ ì¶”ì ", 
    "ğŸ† Top20ê³¼ ë“œë¦´ë‹¤ìš´"
])
# --- TAB 1: ë¹ˆë„ìˆ˜ í†µê³„
with tab1:
    st.subheader("ğŸ“Œ 5ì¤„ ìš”ì•½")
    df_exec = df_exec.iloc[1:].reset_index(drop=True)
    st.markdown(df_exec.iloc[0, 0])

    download_path = f"assets/data/{selected_snapshot}_trend_summary.xlsx"
    try:
        with open(download_path, "rb") as f:
            st.download_button(
                label="ğŸ“¥ ì„ íƒëœ ìŠ¤ëƒ…ìƒ· ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                data=f.read(),
                file_name=f"{selected_snapshot}_trend_summary.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
    except Exception as e:
        st.warning(f"âš ï¸ ë‹¤ìš´ë¡œë“œ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

# --- TAB 2: ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬
with tab2:
    st.subheader("ğŸ•¸ ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬")

    layout_options = {
        "Force-Directed": {"physics": True, "hierarchical": False},
        "Hierarchical - LR": {
            "physics": False,
            "hierarchical": True,
            "layout": {"hierarchical": {"enabled": True, "direction": "LR"}}
        },
        "Hierarchical - TB": {
            "physics": False,
            "hierarchical": True,
            "layout": {"hierarchical": {"enabled": True, "direction": "TB"}}
        },
        "Circular (Random Seed)": {
            "physics": False,
            "hierarchical": False,
            "layout": {"randomSeed": 7}
        }
    }
    # ì‚¬ìš©ì ì„ íƒ ë“œë¡­ë‹¤ìš´
    selected_layout = st.selectbox("ğŸ“ ë„¤íŠ¸ì›Œí¬ ë ˆì´ì•„ì›ƒ ì„ íƒ", list(layout_options.keys()))
    layout_config = layout_options[selected_layout]
    
    # ë…¸ë“œ/ì—£ì§€ êµ¬ì„±
    nodes = []
    for _, row in df_cooccur.iterrows():
        nodes.append(Node(id=row["source"], label=row["source"], font={"color": "white"}))
        nodes.append(Node(id=row["target"], label=row["target"], font={"color": "white"}))
    nodes = {n.id: n for n in nodes}.values()  # ì¤‘ë³µ ì œê±°

    edges = [Edge(source=row.source, target=row.target, label=str(row.count)) for row in df_cooccur.itertuples()]

    # ë„¤íŠ¸ì›Œí¬ config ì„¤ì •
    config = Config(
        width=900,
        height=700,
        nodeHighlightBehavior=True,
        highlightColor="#FFCC00",
        collapsible=True,
        node={"color": "#00BFFF"},
        edge={"color": "#AAAAAA"},
        **layout_config
    )

    try:
        agraph(nodes=nodes, edges=edges, config=config)
    except Exception as e:
        st.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ë Œë”ë§ ì‹¤íŒ¨: {e}")

    st.subheader("ì—°ê´€ì–´ Top 20")
    df_top_assoc = df_assoc.sort_values("count", ascending=False).head(20)
    col1, col2 = st.columns(2)
    for i, row in df_top_assoc.iterrows():
        target_col = col1 if i % 2 == 0 else col2
        target_col.write(f"ğŸ”¹ {row['term']} ({row['count']}íšŒ)")

# --- TAB 3: ì—°ê´€ì–´
with tab3:
    st.subheader("ğŸ“ˆ 7ì¼ ì´ë™ í‰ê·  ê¸°ë°˜ í‚¤ì›Œë“œ íŠ¸ë Œë“œ")

    # ë“œë¡­ë‹¤ìš´: ê·¸ë˜í”„ ìœ í˜• ì„ íƒ
    chart_type = st.selectbox("ğŸ¨ ê·¸ë˜í”„ ìœ í˜• ì„ íƒ", ["ë§‰ëŒ€ê·¸ë˜í”„", "ì„ ê·¸ë˜í”„"])
    
    # í‚¤ì›Œë“œ ì„ íƒ
    selected_keywords = st.multiselect("ğŸ“Œ í‚¤ì›Œë“œ ì„ íƒ", df_rolling.columns.tolist(), default=df_rolling.columns[:5])
    
    if selected_keywords:
        df_long = df_rolling[selected_keywords].reset_index().melt(
            id_vars="Publication Date",
            var_name="Keyword",
            value_name="7d_avg"
        )
    
        # ê·¸ë˜í”„ ìƒì„±
        if chart_type == "ì„ ê·¸ë˜í”„":
            chart = alt.Chart(df_long).mark_line(point=True).encode(
                x="Publication Date:T",
                y="7d_avg:Q",
                color="Keyword:N"
            )
        else:  # ë§‰ëŒ€ê·¸ë˜í”„
            chart = alt.Chart(df_long).mark_bar().encode(
                x="Publication Date:T",
                y="7d_avg:Q",
                color="Keyword:N",
                tooltip=["Publication Date:T", "Keyword:N", "7d_avg:Q"]
            )
    
        st.altair_chart(chart.properties(width=800, height=400), use_container_width=True)

# --- TAB 4: ë³´ê³ ì„œ
with tab4:
    st.subheader("ğŸ“Œ í‚¤ì›Œë“œ Top 20 (ìƒì„¸ ë³´ê¸° í¬í•¨)")

    top_df = df_summary.sort_values("Keyword Count", ascending=False).head(20).copy()
    top_df = top_df.reset_index(drop=True)
    
    # ì»¬ëŸ¼ëª… ì •ë¦¬
    top_df.columns = [c.strip() for c in top_df.columns]
    
    # ìƒˆ í…Œì´ë¸” ë§Œë“¤ê¸°
    table_data = []
    
    for i, row in top_df.iterrows():
        index = i + 1
        keyword = row["Keyword"]
        count = row["Keyword Count"]
        # ë§í¬ ì—´ê¸° (ìƒˆ íƒ­)
        link_html = f'<a href="{row["Source URL"]}" target="_blank">ğŸ”— ë§í¬</a>'
    
        # íˆ´íŒ Summary
        short = row["Short Summary"]
        detailed = row["Detailed Summary"]
        summary_html = f'<span title="{detailed}">{short}</span>'
    
        table_data.append((index, keyword, count, summary_html, link_html))
    
    # í‘œë¥¼ DataFrameìœ¼ë¡œ ì¬ìƒì„± (í‘œì‹œìš©)
    df_display = pd.DataFrame(table_data, columns=["#", "Keyword", "Count", "Summary", "Source"])
    
    # st.markdownì˜ unsafe_allow_htmlë¡œ ë§í¬ì™€ íˆ´íŒ í—ˆìš©
    st.markdown(df_display.to_html(escape=False, index=False), unsafe_allow_html=True)
