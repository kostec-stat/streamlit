# -*- coding: utf-8 -*-
# Author : Prof. Dr. Songhee Kang
# Description : KOSTEC stat visualizer using Excel-based trend summary
# Date : 2025-04-14
# Last Update : 2025-06-14
# **** ì¤‘ê°„ë³´ê³ (2025-05-30) í›„ ìˆ˜ì •ì‚¬í•­: ë„ë„›ê·¸ë˜í”„ ì¶”ê°€, ì—°ê´€ê²€ìƒ‰ì–´ ì„¼í„°ë§, ë§‰ëŒ€ê·¸ë˜í”„ ë‘ê»˜ ì¡°ì •, KOSTEC ë¡œê³  ì‚½ì…
# **** ìµœì¢…ë³´ê³ (2025-06-12) í›„ ìˆ˜ì •ì‚¬í•­: ìŠ¤ëƒ…ìƒ· ì‹œì‘ê³¼ ëì„ ì§€ì •í•´ ê¸°ê°„ë³„ ë³´ê³ ì„œ ë¦¬í¬íŠ¸
# **** ìµœì¢…ë³´ê³ (2025-06-12) í›„ ìˆ˜ì •ì‚¬í•­: ì»¬ëŸ¬íŒ”ë ˆíŠ¸ ì œê³µ
# License : MIT

# --- 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import streamlit as st
import pandas as pd
import altair as alt
from streamlit_agraph import agraph, Node, Edge, Config
from collections import defaultdict
import os, io, zipfile
from datetime import date, datetime
import glob, time
from matplotlib_venn import venn2
import matplotlib.pyplot as plt
import numpy as np
from datetime import timedelta
import html
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer
import itertools

# --- 1. ì„¤ì •
st.set_page_config(page_title="í•œì¤‘ê³¼ê¸°í˜‘ë ¥ì„¼í„° í‚¤ì›Œë“œ ëŒ€ì‹œë³´ë“œ", layout="wide")
col1, col2 = st.columns([2, 8])  # ë¡œê³ :ì œëª© ë¹„ìœ¨ ì¡°ì •
st.markdown("""
    <style>
    .custom-subheader {
        font-size: 25px !important;
        font-weight: 600;
        margin-bottom: 0.5rem;
    }
    </style>
""", unsafe_allow_html=True)
st.markdown("""
    <style>
    [data-testid="stVerticalBlock"] > [data-testid="stHorizontalBlock"] {
        justify-content: center;
    }
    </style>
""", unsafe_allow_html=True)
st.markdown("""
<style>
/* ğŸ¨ íƒ­ ê³µí†µ ìŠ¤íƒ€ì¼ (ê¸€ì”¨ ë³´ì´ê²Œ) */
[data-testid="stTabs"] button {
    font-size: 18px !important;
    font-family: "Noto Sans KR", sans-serif !important;
    padding: 10px 16px !important;
    margin-right: 6px;
    border-radius: 6px;
    color: black !important;   /* âœ… ê¸€ì”¨ ìƒ‰: ê²€ì •ìœ¼ë¡œ ë³€ê²½ */
    font-weight: 500;
}

/* ğŸŒ— ê° íƒ­ì— ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë°°ê²½ìƒ‰ ì ìš© */
[data-testid="stTabs"] button:nth-child(1) {
    background-color: #f0f0f0 !important;  /* ì—°íšŒìƒ‰ */
}
[data-testid="stTabs"] button:nth-child(2) {
    background-color: #d9d9d9 !important;
}
[data-testid="stTabs"] button:nth-child(3) {
    background-color: #bfbfbf !important;
}
[data-testid="stTabs"] button:nth-child(4) {
    background-color: #a6a6a6 !important;
}
[data-testid="stTabs"] button:nth-child(5) {
    background-color: #8c8c8c !important;
}

/* âœ… ì„ íƒëœ íƒ­ ê°•ì¡° ìŠ¤íƒ€ì¼ */
[data-testid="stTabs"] button[aria-selected="true"] {
    border: 2px solid #444 !important;
    font-weight: bold !important;
    margin-bottom: 10px !important;
    box-shadow: 0px 2px 6px rgba(0,0,0,0.15);
}

/* ğŸ§¾ íƒ­ ë‚´ë¶€ ê¸€ì í¬ê¸° ë° íŒ¨ë”© í™•ëŒ€ */
.block-container {
    font-size: 17px !important;
    font-family: "Noto Sans KR", sans-serif !important;
    padding: 1.5rem 2rem !important;
}
</style>
""", unsafe_allow_html=True)

with col1:
    st.image("assets/images/logo.svg")  # ë¡œê³  íŒŒì¼ ê²½ë¡œì™€ í¬ê¸° ì„¤ì •

with col2:
    st.markdown("""
        <h1 style='font-size:35px; color:#044B9A; padding-top: 6px;'>
        í•œì¤‘ê³¼ê¸°í˜‘ë ¥ì„¼í„° í‚¤ì›Œë“œ ë™í–¥ ëŒ€ì‹œë³´ë“œ
        </h1>
    """, unsafe_allow_html=True)

# --- 2. CSS ì ìš©
def local_css(file_name):
    with open(file_name, "r", encoding="utf-8") as f:
        css_content = f.read()
    st.markdown(f"<style>{css_content}</style>", unsafe_allow_html=True)

local_css("assets/css/main.css")

# --- 3. ì‚¬ì´ë“œë°” 
color_palettes = [
    "viridis", "plasma", "magma", "inferno", "turbo",
    "category10", "category20", "accent", "dark2", "set1", "set2", "set3"
]

# ğŸ‘‰ ì‚¬ì´ë“œë°”ì—ì„œ íŒ”ë ˆíŠ¸ ì„ íƒ
selected_palette = st.sidebar.selectbox("ğŸ¨ ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì„ íƒ", color_palettes, index=0)
start_date = st.sidebar.date_input("ğŸ—“ ì‹œì‘ì¼", value=date.today() - timedelta(days=7), key="start_date")
end_date = st.sidebar.date_input("â³ ì¢…ë£Œì¼", value=date.today(), key="end_date")

# íŒŒì¼ ëª©ë¡ í•„í„°ë§ (ì¤‘êµ­ ë²„ì „ ê¸°ì¤€)
snapshot_files = glob.glob("assets/data/*_trend_summary.xlsx")
snapshot_files_global = glob.glob("assets/data/*_trend_summary_en.xlsx")
snapshot_dates = [os.path.basename(f).split("_")[0] for f in snapshot_files]
snapshot_dates = [d for d in snapshot_dates if d.isdigit() and len(d) == 8]

# ë‚ ì§œ í•„í„°ë§
selected_files = [
    f for f in snapshot_files
    if start_date.strftime("%Y%m%d") <= os.path.basename(f).split("_")[0] <= end_date.strftime("%Y%m%d")
]
selected_files_global = [
    f for f in snapshot_files_global
    if start_date.strftime("%Y%m%d") <= os.path.basename(f).split("_")[0] <= end_date.strftime("%Y%m%d")
]

st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ›° ì£¼ê°„ ë™í–¥ ìˆ˜ì§‘")

input_date = st.sidebar.date_input("ğŸ“† ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ", value=date.today(), key="expander_date")
current_date = input_date.strftime("%Y%m%d")
api_token = st.sidebar.text_input("ğŸ” Claude API í† í°", type="password", key="expander_api")
github_token = st.sidebar.text_input("ğŸªª GitHub Token", type="password", key="expander_git")

if st.sidebar.button("ìˆ˜ì§‘ ì‹œì‘(ì¤‘êµ­) ğŸš€ ", key="expander_run1"):
    with st.spinner(f"ğŸ“¡ {current_date} ê¸°ì¤€ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤... ìµœëŒ€ 3~5ë¶„ ì†Œìš”."):
        try:
            import os
            import anthropic
            import re
            from io import StringIO
            from itertools import combinations
            from openpyxl import load_workbook

            client = anthropic.Anthropic(api_key=api_token)

            with open("assets/input/keywords.txt", "r", encoding="utf-8") as f:
                keywords = f.read().strip()
            with open("assets/input/sites.txt", "r", encoding="utf-8") as f:
                source_sites = f.read().strip()
            with open("assets/input/prompt.txt", "r", encoding="utf-8") as f:
                prompt_template = f.read()

                            # ë³€ìˆ˜ ì •ì˜
            prompt1 = prompt_template.format(
                keywords=keywords,        # ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ joiní•œ ê°’
                current_date=current_date,        # '20250518' ê°™ì€ ë¬¸ìì—´
                source_sites=source_sites        # ë¬¸ìì—´ ë˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡
            )
                        #st.write(prompt2)
                            # Claude API í˜¸ì¶œ
            message = client.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=20000,
        temperature=1,
        messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt1
                            }
                        ]
                    }
                ]
      )

                        #st.write(prompt1)
            st.write("Step 1: RAG ìˆ˜í–‰ ì™„ë£Œ.")
                            # ê²°ê³¼ íŒŒì‹±
            text_data = message.content[0].text if isinstance(message.content, list) else message.content.text
            match = re.search(r"<excel_report>(.*?)</excel_report>", text_data, re.DOTALL)
            text_block = match.group(0) if match else None
            st.write("Step 2: ë©”ì‹œì§€ íŒŒì‹± ì™„ë£Œ.")
            sheet1_start = text_block.find("<sheet1>")
            sheet1_end = text_block.find("</sheet1>")
            sheet2_start = text_block.find("<sheet2>")
            sheet2_end = text_block.find("</sheet2>")
            summary_start = text_block.find("<executive_summary>")
            summary_end = text_block.find("</executive_summary>")

            sheet1_text = text_block[sheet1_start:sheet1_end]
            sheet2_text = text_block[sheet2_start:sheet2_end]
            executive_summary_text = text_block[summary_start + len("<executive_summary>"):summary_end].strip()

            st.write("Step 3: ì—‘ì…€ ì‹œíŠ¸ íŒŒì‹± ì™„ë£Œ")

            sheet1_table_match = re.search(r"(\|.+?\|\n\|[-|]+\|\n(.+?))$", sheet1_text, re.DOTALL)
            sheet2_table_match = re.search(r"(\|.+?\|\n\|[-|]+\|\n(.+?))$", sheet2_text, re.DOTALL)

            sheet1_table_md = sheet1_table_match.group(1).strip() if sheet1_table_match else ""
            sheet2_table_md = sheet2_table_match.group(1).strip() if sheet2_table_match else ""

            df_sheet1 = pd.read_csv(StringIO(sheet1_table_md), sep="|", engine="python").dropna(axis=1, how="all")
            df_sheet2 = pd.read_csv(StringIO(sheet2_table_md), sep="|", engine="python").dropna(axis=1, how="all")
                            # ì €ì¥
            excel_path = f"assets/data/{current_date}_trend_summary.xlsx"
                            # ë™ì‹œì¶œí˜„ ë° ì—°ê´€ì–´ ë¶„ì„
            df_summary = df_sheet1.iloc[1:].reset_index(drop=True)
            df_summary.columns = [col.strip() for col in df_summary.columns]
            keywords_list = [kw.strip() for kw in df_summary["Keyword"].dropna().unique().tolist()]

            cooccur_counter = defaultdict(int)
            association_counter = defaultdict(int)
            for _, row in df_summary.iterrows():
                text = (str(row.get("Detailed Summary", "")) + " " + str(row.get("Short Summary", ""))).lower()
                present_keywords = [kw for kw in keywords_list if kw.lower() in text]
                for kw1, kw2 in combinations(sorted(set(present_keywords)), 2):
                    cooccur_counter[(kw1, kw2)] += 1
                for kw in present_keywords:
                    association_counter[kw] += 1

            df_cooccur = pd.DataFrame([{"source": k1, "target": k2, "count": v} for (k1, k2), v in cooccur_counter.items()])
            df_association = pd.DataFrame([{"term": k, "count": v} for k, v in association_counter.items()])
            st.write("Step 4: ë™ì‹œì¶œí˜„ ë° ì—°ê´€ì–´ ë¶„ì„ ì™„ë£Œ")

            with pd.ExcelWriter(excel_path, engine="openpyxl", mode="w") as writer:
                df_summary.to_excel(writer, index=False, sheet_name="Summary Table")
                df_sheet2.to_excel(writer, index=False, sheet_name="Sources")
                pd.DataFrame({"Executive Summary": [executive_summary_text]}).to_excel(writer, index=False, sheet_name="Executive Summary")
                df_cooccur.to_excel(writer, index=False, sheet_name="Cooccurrence")
                df_association.to_excel(writer, index=False, sheet_name="Associations")

            st.sidebar.success(f"{current_date} ê¸°ì¤€ ì£¼ê°„ ì¤‘êµ­ ë™í–¥ ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ!")

        except Exception as e:
            st.sidebar.error(f"âŒ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        from github import Github
        repo_name = "kostec-stat/streamlit"
        file_path = f"assets/data/{current_date}_trend_summary.xlsx"

        try:
            g = Github(github_token)
            repo = g.get_repo(repo_name)

            with open(file_path, "rb") as f:
                content = f.read()
                path_in_repo = f"assets/data/{current_date}_trend_summary.xlsx"

            try:
                existing_file = repo.get_contents(path_in_repo)
                repo.update_file(existing_file.path, f"update {path_in_repo}", content, existing_file.sha)
            except Exception:
                repo.create_file(path_in_repo, f"add {path_in_repo}", content)

            st.success(f" {current_date} ê¸°ì¤€ ì£¼ê°„ ë™í–¥ ìˆ˜ì§‘, ì €ì¥ ë° GitHub ì—…ë¡œë“œ ì™„ë£Œ!")
        except Exception as upload_err:
            st.warning(f"âš ï¸ ìˆ˜ì§‘ì€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ GitHub ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_err}")

if st.sidebar.button("ìˆ˜ì§‘ ì‹œì‘(ê¸€ë¡œë²Œ) ğŸš€ ", key="expander_run2"):
    with st.spinner("â³ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤. ìµœëŒ€ 3~5ë¶„ ì†Œìš”..."):
        try:
            import os
            import anthropic
            import re
            from io import StringIO
            from itertools import combinations
            from openpyxl import load_workbook
                            # API ì—°ê²°
            client = anthropic.Anthropic(api_key=api_token)
            with open("assets/input/en_keywords.txt", "r", encoding="utf-8") as f:
                en_keywords = f.read().strip()
            with open("assets/input/prompt.txt", "r", encoding="utf-8") as f:
                prompt_template = f.read()
                            # ë³€ìˆ˜ ì •ì˜
            prompt2 = prompt_template.format(
                keywords=en_keywords,        # ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ joiní•œ ê°’
                current_date=current_date,        # '20250518' ê°™ì€ ë¬¸ìì—´
                source_sites="*"        # ë¬¸ìì—´ ë˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡
            )
                        #st.write(prompt2)
                            # Claude API í˜¸ì¶œ
            message = client.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=20000,
        temperature=1,
        messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt2
                            }
                        ]
                    }
                ]
      )

                        #st.write(prompt1)
            st.write("Step 1: RAG ìˆ˜í–‰ ì™„ë£Œ.")

                            # ê²°ê³¼ íŒŒì‹±
            text_data = message.content[0].text if isinstance(message.content, list) else message.content.text
            match = re.search(r"<excel_report>(.*?)</excel_report>", text_data, re.DOTALL)
            text_block = match.group(0) if match else None
            st.write("Step 2: ë©”ì‹œì§€ íŒŒì‹± ì™„ë£Œ.")
            sheet1_start = text_block.find("<sheet1>")
            sheet1_end = text_block.find("</sheet1>")
            sheet2_start = text_block.find("<sheet2>")
            sheet2_end = text_block.find("</sheet2>")
            summary_start = text_block.find("<executive_summary>")
            summary_end = text_block.find("</executive_summary>")

            sheet1_text = text_block[sheet1_start:sheet1_end]
            sheet2_text = text_block[sheet2_start:sheet2_end]
            executive_summary_text = text_block[summary_start + len("<executive_summary>"):summary_end].strip()
            sheet1_table_match = re.search(r"(\|.+?\|\n\|[-|]+\|\n(.+?))$", sheet1_text, re.DOTALL)
            sheet2_table_match = re.search(r"(\|.+?\|\n\|[-|]+\|\n(.+?))$", sheet2_text, re.DOTALL)

            sheet1_table_md = sheet1_table_match.group(1).strip() if sheet1_table_match else ""
            sheet2_table_md = sheet2_table_match.group(1).strip() if sheet2_table_match else ""

            df_sheet1 = pd.read_csv(StringIO(sheet1_table_md), sep="|", engine="python").dropna(axis=1, how="all")
            df_sheet2 = pd.read_csv(StringIO(sheet2_table_md), sep="|", engine="python").dropna(axis=1, how="all")
            st.write("Step 3: ì—‘ì…€ ì‹œíŠ¸ íŒŒì‹± ì™„ë£Œ")
                            # ì €ì¥
            excel_path = f"assets/data/{current_date}_trend_summary_en.xlsx"
                            # ë™ì‹œì¶œí˜„ ë° ì—°ê´€ì–´ ë¶„ì„
            df_summary = df_sheet1.iloc[1:].reset_index(drop=True)
            df_summary.columns = [col.strip() for col in df_summary.columns]
            keywords_list = [kw.strip() for kw in df_summary["Keyword"].dropna().unique().tolist()]

            cooccur_counter = defaultdict(int)
            association_counter = defaultdict(int)
            for _, row in df_summary.iterrows():
                text = (str(row.get("Detailed Summary", "")) + " " + str(row.get("Short Summary", ""))).lower()
                present_keywords = [kw for kw in keywords_list if kw.lower() in text]
                for kw1, kw2 in combinations(sorted(set(present_keywords)), 2):
                    cooccur_counter[(kw1, kw2)] += 1
                for kw in present_keywords:
                    association_counter[kw] += 1

            df_cooccur = pd.DataFrame([{"source": k1, "target": k2, "count": v} for (k1, k2), v in cooccur_counter.items()])
            df_association = pd.DataFrame([{"term": k, "count": v} for k, v in association_counter.items()])
            st.write("Step 4: ë™ì‹œì¶œí˜„ ë° ì—°ê´€ì–´ ë¶„ì„ ì™„ë£Œ")
            with pd.ExcelWriter(excel_path, engine="openpyxl", mode="w") as writer:
                df_summary.to_excel(writer, index=False, sheet_name="Summary Table")
                df_sheet2.to_excel(writer, index=False, sheet_name="Sources")
                pd.DataFrame({"Executive Summary": [executive_summary_text]}).to_excel(writer, index=False, sheet_name="Executive Summary")
                df_cooccur.to_excel(writer, index=False, sheet_name="Cooccurrence")
                df_association.to_excel(writer, index=False, sheet_name="Associations")

            st.sidebar.success(f"{current_date} ê¸°ì¤€ ì£¼ê°„ ê¸€ë¡œë²Œ ë™í–¥ ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ!")

        except Exception as e:
            st.sidebar.error(f"âŒ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        from github import Github
        repo_name = "kostec-stat/streamlit"
        file_path = f"assets/data/{current_date}_trend_summary_en.xlsx"

        try:
            g = Github(github_token)
            repo = g.get_repo(repo_name)

            with open(file_path, "rb") as f:
                content = f.read()
                path_in_repo = f"assets/data/{current_date}_trend_summary_en.xlsx"

            try:
                existing_file = repo.get_contents(path_in_repo)
                repo.update_file(existing_file.path, f"update {path_in_repo}", content, existing_file.sha)
            except Exception:
                repo.create_file(path_in_repo, f"add {path_in_repo}", content)

            st.success(f" {current_date} ê¸°ì¤€ ì£¼ê°„ ë™í–¥ ìˆ˜ì§‘, ì €ì¥ ë° GitHub ì—…ë¡œë“œ ì™„ë£Œ!")
        except Exception as upload_err:
            st.warning(f"âš ï¸ ìˆ˜ì§‘ì€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ GitHub ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_err}")

@st.cache_data
def load_excel_data(path):
    xls = pd.ExcelFile(path)
    df_summary = pd.read_excel(xls, sheet_name="Summary Table")
    df_sources = pd.read_excel(xls, sheet_name="Sources")
    df_exec = pd.read_excel(xls, sheet_name="Executive Summary")
    df_cooccur = pd.read_excel(xls, sheet_name="Cooccurrence")
    df_assoc = pd.read_excel(xls, sheet_name="Associations")
    return df_summary, df_sources, df_exec, df_cooccur, df_assoc

# ëª¨ë“  íŒŒì¼ì—ì„œ ì‹œíŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
df_summary_all, df_sources_all, df_exec_all, df_cooccur_all, df_assoc_all = [], [], [], [], []

for path in selected_files:
    try:
        df_s, df_src, df_exe, df_co, df_as = load_excel_data(path)
        df_summary_all.append(df_s)
        df_sources_all.append(df_src)
        df_exec_all.append(df_exe)
        df_cooccur_all.append(df_co)
        df_assoc_all.append(df_as)
    except Exception as e:
        st.warning(f"âš ï¸ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {path}, ì˜¤ë¥˜: {e}")

# í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í†µí•©
if df_summary_all:
    df_summary = pd.concat(df_summary_all, ignore_index=True)
    df_sources = pd.concat(df_sources_all, ignore_index=True)
    df_exec = pd.concat(df_exec_all, ignore_index=True)
    df_cooccur = pd.concat(df_cooccur_all, ignore_index=True)
    df_assoc = pd.concat(df_assoc_all, ignore_index=True)
else:
    st.error("âŒ ì„ íƒí•œ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

df_summary.columns = [col.strip() for col in df_summary.columns]
df_cooccur.columns = [col.strip() for col in df_cooccur.columns]
df_sources.columns = [col.strip() for col in df_sources.columns]

# 2. ì¡´ì¬ ì—¬ë¶€ í™•ì¸
required_cols = {"URL", "Publication Date"}
missing_cols = required_cols - set(df_sources.columns)

if missing_cols:
    st.error(f"âŒ df_sourcesì— ë‹¤ìŒ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}")
    st.write("ğŸ“Œ í˜„ì¬ ì»¬ëŸ¼ ëª©ë¡:", df_sources.columns.tolist())
    st.stop()
else:
    df_merged = df_summary.merge(
        df_sources[["URL", "Publication Date"]],
        how="left",
        left_on="Source URL",
        right_on="URL"
    )
    
if "count" not in df_cooccur.columns:
    st.error("âŒ 'count' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.write("ğŸ“Œ í˜„ì¬ ì»¬ëŸ¼:", df_cooccur.columns.tolist())
    st.stop()

# ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ì¸ì§€ í™•ì¸
if "Keyword Count" not in df_summary.columns:
    st.error("âŒ 'Keyword Count' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.write("ğŸ” í˜„ì¬ ì»¬ëŸ¼ ëª©ë¡:", df_summary.columns.tolist())
    st.stop()

# URL ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ë§¤í•‘
df_merged = df_summary.merge(
    df_sources[["URL", "Publication Date"]],
    how="left",
    left_on="Source URL",
    right_on="URL"
)
# ë‚ ì§œ ì •ë¦¬
df_merged["Publication Date"] = pd.to_datetime(df_merged["Publication Date"])
df_merged["Keyword"] = df_merged["Keyword"].astype(str)

# ì¼ìë³„ í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ ì§‘ê³„
df_daily = df_merged.groupby(["Publication Date", "Keyword"]).size().reset_index(name="count")

# í”¼ë²— í…Œì´ë¸”ë¡œ ì¼ì x í‚¤ì›Œë“œ í˜•íƒœ
df_pivot = df_daily.pivot_table(index="Publication Date", columns="Keyword", values="count", fill_value=0).sort_index()

# 7ì¼ ì´ë™ í‰ê· 
df_rolling = df_pivot.rolling(window=7, min_periods=1).mean()

# --- 4. íƒ­ êµ¬ì„±
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "ğŸ“Š ìš”ì•½ê³¼ ë‹¤ìš´ë¡œë“œ", 
    "ğŸ•¸ ë™ì‹œì¶œí˜„ê³¼ ì—°ê´€ì–´", 
    "ğŸ” ë¹ˆë„ìˆ˜ ì¶”ì ", 
    "ğŸ† Top20ê³¼ ë“œë¦´ë‹¤ìš´",
    "ğŸŒ ê¸€ë¡œë²Œ ë¹„êµ"
])
# --- TAB 1: ë¹ˆë„ìˆ˜ í†µê³„
with tab1:
    st.markdown("<div class='custom-subheader'>ğŸ“Œ ì£¼ìš” ìš”ì•½ </div>", unsafe_allow_html=True)
    if not df_exec.empty and df_exec.shape[1] > 0:
        df_exec.columns = [c.strip() for c in df_exec.columns]
    # ëª¨ë“  ì…€ì„ ë¬¸ìì—´ë¡œ í•©ì¹œ í›„, '1.' ì´í›„ ì¶”ì¶œ
        full_text = "\n".join(df_exec.iloc[:, 0].astype(str).tolist())
        start_index = full_text.find("1.")
        if start_index != -1:
            cleaned_summary = full_text[start_index:].strip()
            #try:
            #    parser = PlaintextParser.from_string(full_text, Tokenizer("chinese"))
            #    summarizer = TextRankSummarizer()
            #    summary_sentences = summarizer(parser.document, 5)  # ìµœëŒ€ 5ë¬¸ì¥
                
            #    if summary_sentences:
            #        for i, sentence in enumerate(summary_sentences, 1):
            #            st.markdown(f"**{i}.** {sentence}")
            #   else:
            #        st.info("â„¹ï¸ ìš”ì•½í•  ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            #except Exception as e:
            #    st.error(f"âŒ ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            st.markdown(cleaned_summary)
        else:
            st.warning("âš ï¸ '1.'ë¡œ ì‹œì‘í•˜ëŠ” ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ Executive Summary ì‹œíŠ¸ê°€ ë¹„ì–´ ìˆê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    st.markdown("<div class='custom-subheader'>ğŸ“ ë‹¤ìš´ë¡œë“œí•  ìŠ¤ëƒ…ìƒ· ì„ íƒ</div>", unsafe_allow_html=True)

    # ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆëŠ” ìŠ¤ëƒ…ìƒ· ëª©ë¡ êµ¬ì„± (ê¸°ì¡´ snapshot íŒŒì¼ ê¸°ì¤€)
    all_snapshot_files = glob.glob("assets/data/*_trend_summary.xlsx")
    snapshot_options = sorted({os.path.basename(f).split("_trend_summary")[0] for f in all_snapshot_files}, reverse=True)
    
    selected_download_snapshot = st.selectbox("ğŸ“… ë‹¤ìš´ë¡œë“œí•  ë‚ ì§œ ì„ íƒ", snapshot_options)
    
    col1, col2 = st.columns(2)
    with col1:
        china_file = f"assets/data/{selected_download_snapshot}_trend_summary.xlsx"
        try:
            with open(china_file, "rb") as f:
                st.download_button(
                    label=f"ğŸ“¥ {selected_download_snapshot} ì¤‘êµ­ ì£¼ê°„ë™í–¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=f.read(),
                    file_name=f"{selected_download_snapshot}_trend_summary.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        except Exception as e:
            st.warning(f"âš ï¸ ì¤‘êµ­ ìŠ¤ëƒ…ìƒ· íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    with col2:
        global_file = f"assets/data/{selected_download_snapshot}_trend_summary_en.xlsx"
        try:
            with open(global_file, "rb") as f:
                st.download_button(
                    label=f"ğŸ“¥ {selected_download_snapshot} ê¸€ë¡œë²Œ ì£¼ê°„ë™í–¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                    data=f.read(),
                    file_name=f"{selected_download_snapshot}_trend_summary_en.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        except Exception as e:
            st.warning(f"âš ï¸ ê¸€ë¡œë²Œ ìŠ¤ëƒ…ìƒ· íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
      
# --- TAB 2: ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬
with tab2:
    if "graph_first_rendered" not in st.session_state:
        st.session_state.graph_first_rendered = False

    if not st.session_state.graph_first_rendered:
        # ê°•ì œë¡œ ë ˆì´ì•„ì›ƒì„ í•œ ë²ˆ ë°”ê¿¨ë‹¤ê°€ ì›ë˜ëŒ€ë¡œ ëŒë¦¼
        st.session_state.graph_first_rendered = True
        st.rerun()
        
    st.markdown("<div class='custom-subheader'>ğŸ•¸ ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬</div>", unsafe_allow_html=True)

    layout_options = {
        "Static (ì¢Œí‘œê³ ì •)": {
            "improvedLayout": False,
            "physics": False,
            "hierarchical": False,
            "staticGraph": True
        },
        "Circular (Centered)": {
            "improvedLayout": True,
            "randomSeed": 42,
            "physics": False,
            "hierarchical": False,
            "layout": {"hierarchical": {"enabled": False}},
        },
        "Force-Directed (Spring)": {
            "improvedLayout": True,
            "physics": False,
            "hierarchical": False,
        },
        "Random": {
            "improvedLayout": False,
            "physics": False,
            "hierarchical": False,
            "layout": {"randomSeed": 42},
        },
        "Grid": {
            "improvedLayout": False,
            "physics": False,
            "hierarchical": False,
            "layout": {"grid": {"enabled": True}},
        },
        "Hierarchical - LR": {
            "improvedLayout": True,
            "randomSeed": 42,
            "physics": False,
            "hierarchical": True,
            "layout": {"hierarchical": {"enabled": True, "direction": "LR"}}
        },
        "Hierarchical - RL": {
            "improvedLayout": True,
            "randomSeed": 42,
            "physics": False,
            "hierarchical": True,
            "layout": {"hierarchical": {"enabled": True, "direction": "RL"}}
        },
        "Hierarchical - TB": {
            "improvedLayout": True,
            "randomSeed": 42,
            "physics": False,
            "hierarchical": True,
            "layout": {"hierarchical": {"enabled": True, "direction": "TB"}}
        },
        "Hierarchical - BT": {
            "improvedLayout": True,
            "randomSeed": 42,
            "physics": False,
            "hierarchical": True,
            "layout": {"hierarchical": {"enabled": True, "direction": "BT"}}
        }
    }

    # ğŸ“Œ ê³ ì • íšŒìƒ‰ ìŠ¤ì¼€ì¼ íŒ”ë ˆíŠ¸
    gray_palette = [
        "#111111", "#2c2c2c", "#444444", "#666666", "#888888",
        "#aaaaaa", "#cccccc", "#dddddd", "#eeeeee"
    ]
    color_cycle = itertools.cycle(gray_palette)
    
    # 1. ë ˆì´ì•„ì›ƒ êµ¬ì„±
    selected_layout = st.selectbox("ğŸ“ ë„¤íŠ¸ì›Œí¬ ë ˆì´ì•„ì›ƒ ì„ íƒ", list(layout_options.keys()))
    layout_config = layout_options[selected_layout]
    
    # 2. ë…¸ë“œ êµ¬ì„±
    unique_nodes = set(df_cooccur["source"]).union(set(df_cooccur["target"]))
    node_color_map = {node: next(color_cycle) for node in unique_nodes}
    
    nodes = [
        Node(id=node, label=node, color=node_color_map[node], font={"color": "darkgray"})
        for node in unique_nodes
    ]
    
    # 4. ì—£ì§€ êµ¬ì„±
    edges = [
        Edge(source=row.source, target=row.target, label=str(row.count))
        for row in df_cooccur.itertuples()
    ]
    
    # 5. ê·¸ë˜í”„ êµ¬ì„± ì˜µì…˜
    config = Config(
        width=800,
        height=600,
        layout=layout_config,
        physics=True,
        staticGraph=False
    )
    
    # 6. ë Œë”ë§
    try:
        agraph(nodes=nodes, edges=edges, config=config)
    except Exception as e:
        st.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ë Œë”ë§ ì‹¤íŒ¨: {e}")

    st.subheader("ì—°ê´€ì–´ Top 20")
    df_top_assoc = df_assoc.sort_values("count", ascending=False).head(20)
    col1, col2 = st.columns(2)
    for i, row in df_top_assoc.iterrows():
        target_col = col1 if i % 2 == 0 else col2
        target_col.write(f"ğŸ”¹ {row['term']} ({row['count']}íšŒ)")

# --- TAB 3: ë¹ˆë„ìˆ˜ ì¶”ì 
with tab3:
    st.markdown("<div class='custom-subheader'>ğŸ“ˆ 7ì¼ ì´ë™ í‰ê·  ê¸°ë°˜ í‚¤ì›Œë“œ íŠ¸ë Œë“œ</div>", unsafe_allow_html=True)

    chart_type = st.selectbox("ğŸ¨ ê·¸ë˜í”„ ìœ í˜• ì„ íƒ", ["ë§‰ëŒ€ê·¸ë˜í”„", "ì„ ê·¸ë˜í”„", "ë„ë„›í˜• ê·¸ë˜í”„"])
    selected_keywords = st.multiselect("ğŸ“Œ í‚¤ì›Œë“œ ì„ íƒ", df_rolling.columns.tolist(), default=df_rolling.columns[:5])
    color_scheme = alt.Scale(scheme=selected_palette)
    
    if selected_keywords:
      df_long = df_rolling[selected_keywords].reset_index().melt(
        id_vars="Publication Date",
        var_name="Keyword",
        value_name="7d_avg"
      )

      if chart_type == "ì„ ê·¸ë˜í”„":
        chart = alt.Chart(df_long).mark_line(point=True).encode(
            x="Publication Date:T",
            y="7d_avg:Q",
            color=alt.Color("Keyword:N", scale=color_scheme)
        ).properties(width=800)
        st.altair_chart(chart, use_container_width=True)

      elif chart_type == "ë§‰ëŒ€ê·¸ë˜í”„":
        chart = alt.Chart(df_long).mark_bar(size=30).encode(
            x=alt.X("Publication Date:T", axis=alt.Axis(labelAngle=-45)),
            y="7d_avg:Q",
            color=alt.Color("Keyword:N", scale=color_scheme),
            tooltip=["Publication Date:T", "Keyword:N", "7d_avg:Q"]
        ).properties(width=800)
        st.altair_chart(chart, use_container_width=True)

      elif chart_type == "ë„ë„›í˜• ê·¸ë˜í”„":
        st.markdown("### ğŸ© ì„ íƒ í‚¤ì›Œë“œ ë¹„ì¤‘")
        # ìµœê·¼ 7ì¼ ê¸°ì¤€ ë°ì´í„° ì§‘ê³„
        #latest_date = df_long["Publication Date"].max()
        #start_date = latest_date - timedelta(days=6)
        #recent_data = df_long[df_long["Publication Date"] >= start_date]

        # í‚¤ì›Œë“œë³„ ì´í•©
        keyword_totals = df_long.groupby("Keyword")["7d_avg"].sum()
        keyword_totals = keyword_totals[keyword_totals > 0]

        if keyword_totals.empty:
            st.warning("ğŸ“­ ìœ íš¨í•œ í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # Altairìš© DataFrame ìƒì„±
            labels = keyword_totals.index.tolist()
            values = keyword_totals.values.tolist()
            label_texts = [f"{kw} ({val:.2f})" for kw, val in zip(labels, values)]
            keyword_totals_df = pd.DataFrame({
                "Keyword": labels,
                "Value": values,
                "LabelText": label_texts
            })
        
            donut = alt.Chart(keyword_totals_df).mark_arc(innerRadius=50, outerRadius=100).encode(
                theta=alt.Theta(field="Value", type="quantitative"),
                color=alt.Color(field="Keyword", scale=color_scheme),
                tooltip=[alt.Tooltip("Keyword"), alt.Tooltip("Value")]
            )
            st.altair_chart(donut, use_container_width=True)
            
# --- TAB 4: í‚¤ì›Œë“œ Top 20 ìƒì„¸ ë³´ê¸° í¬í•¨
with tab4:
    st.markdown("<div class='custom-subheader'>ğŸ“Œ í‚¤ì›Œë“œ Top 20 (ìƒì„¸ ë³´ê¸°)</div>", unsafe_allow_html=True)

    df_summary.columns = [col.strip() for col in df_summary.columns]

    # âœ… groupbyë¡œ Count ì§‘ê³„ + ëŒ€í‘œ ìš”ì•½ + ë§í¬ ëª¨ìŒ
    grouped = (
        df_summary
        .groupby("Keyword")
        .agg({
            "Keyword Count": "sum",
            "Short Summary": lambda x: x.dropna().iloc[0] if not x.dropna().empty else "",
            "Detailed Summary": lambda x: x.dropna().iloc[0] if not x.dropna().empty else "",
            "Source URL": lambda urls: list(set(filter(lambda u: isinstance(u, str), urls)))
        })
        .reset_index()
        .sort_values("Keyword Count", ascending=False)
        .head(20)
    )

    table_data = []
    for i, row in grouped.iterrows():
        index = i + 1
        keyword = row["Keyword"]
        count = row["Keyword Count"]

        summary_html = f'<span title="{html.escape(row["Detailed Summary"])}">{html.escape(row["Short Summary"])}' \
                       f'</span>'

        # ì—¬ëŸ¬ ë§í¬ ëª¨ì•„ì„œ í•œ ì¤„ì— í‘œì‹œ
        urls = row["Source URL"]

        # ë¬¸ìì—´ì¸ ê²½ìš° (ë‹¨ì¼ URL), ë¦¬ìŠ¤íŠ¸ë¡œ ê°ìŒˆ
        if isinstance(urls, str):
            urls = [urls]
        elif not isinstance(urls, list):
            urls = []
        
        # ë§í¬ HTML ìƒì„±
        link_html = f'<a href="{urls[0]}" target="_blank">ğŸ”— link</a>'
        table_data.append((index, keyword, count, summary_html, link_html))
        
    df_display = pd.DataFrame(table_data, columns=["#", "Keyword", "Count", "Summary", "Sources"])
    st.markdown(df_display.to_html(escape=False, index=False), unsafe_allow_html=True)
    
with tab5:
    st.markdown("<div class='custom-subheader'>ğŸ… ì¤‘êµ­ vs ê¸€ë¡œë²Œ í‚¤ì›Œë“œ ìˆœìœ„ ë¹„êµ</div>", unsafe_allow_html=True)

    # 1. í‚¤ì›Œë“œ ë§¤í•‘ í…Œì´ë¸” ìƒì„±
    with open("assets/input/keywords.txt", "r", encoding="utf-8") as f:
        zh_keywords = [line.strip() for line in f if line.strip()]
    with open("assets/input/en_keywords.txt", "r", encoding="utf-8") as f:
        en_keywords = [line.strip() for line in f if line.strip()]

    df_map = pd.DataFrame({
        "zh_keyword": zh_keywords,
        "en_keyword": en_keywords
    })
    #st.write(df_map)
    # 2. êµ­ë‚´ Summary Table
    df_summary.columns = [col.strip() for col in df_summary.columns]
    zh_set = set(df_summary["Keyword"])

    # 3. ê¸€ë¡œë²Œ Summary Table
    df_summary_global_all = []

    for path in selected_files_global:
        try:
            xls = pd.ExcelFile(path)
            df_s = pd.read_excel(xls, sheet_name="Summary Table")
            df_summary_global_all.append(df_s)
        except Exception as e:
            st.warning(f"âš ï¸ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {path}, ì˜¤ë¥˜: {e}")
    
    # í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í†µí•©
    if df_summary_global_all:
        df_global_summary = pd.concat(df_summary_global_all, ignore_index=True)
    else:
        st.error("âŒ ì„ íƒí•œ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    
    df_global_summary.columns = [col.strip() for col in df_global_summary.columns]

    # 4. ê¸€ë¡œë²Œ í‚¤ì›Œë“œ ë§¤í•‘ (ì˜ë¬¸ â†’ ì¤‘ë¬¸)
    map_dict = {
        en.strip().lower(): zh.strip()
        for en, zh in zip(df_map["en_keyword"], df_map["zh_keyword"])
    }
    
    # df_global_summaryì˜ Keywordë„ ì •ê·œí™”í•´ì„œ ë§¤í•‘
    df_global_summary["zh_keyword"] = df_global_summary["Keyword"].str.strip().str.lower().map(map_dict)
    
    matched_zh = set(df_global_summary["zh_keyword"].dropna())
    # 1. êµ­ë‚´ ìˆœìœ„í‘œ
    df_rank_china = (
        df_summary
        .groupby("Keyword", as_index=False)["Keyword Count"].sum()
        .assign(Rank_China=lambda df: df["Keyword Count"].rank(ascending=False, method="min").astype(int))
        .sort_values("Rank_China")
        [["Rank_China", "Keyword", "Keyword Count"]]
    )
    
    # 2. ê¸€ë¡œë²Œ ìˆœìœ„í‘œ (zh_keyword ê¸°ì¤€)
    df_rank_global = (
        df_global_summary
        .groupby("zh_keyword", as_index=False)["Keyword Count"].sum()
        .assign(Rank_Global=lambda df: df["Keyword Count"].rank(ascending=False, method="min").astype(int))
        .sort_values("Rank_Global")
        .rename(columns={"zh_keyword": "Keyword"})
        [["Rank_Global", "Keyword", "Keyword Count"]]
    )
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ‡¨ğŸ‡³ ì¤‘êµ­ í‚¤ì›Œë“œ ìˆœìœ„ (Rank_China)")
        html_china = df_rank_china.reset_index(drop=True).to_html(index=False, escape=False)
        st.markdown(html_china, unsafe_allow_html=True)

    with col2:
        st.markdown("#### ğŸŒ ê¸€ë¡œë²Œ í‚¤ì›Œë“œ ìˆœìœ„ (Rank_Global)")
        html_global = df_rank_global.reset_index(drop=True).to_html(index=False, escape=False)
        st.markdown(html_global, unsafe_allow_html=True)
